{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Music_Transformer_Public.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyNLPVdZi5+RM3SVX40YsVps",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/spectraldoy/MusicTransformerTensorFlow/blob/main/Music_Transformer_Public.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hFC_bRgHQzKI"
      },
      "source": [
        "### Â© Copyright 2021 Aditya Gomatam\n",
        "\n",
        "Licensed under the Apache License, Version 2.0 (the \"License\");"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YYEbY6bES-hT"
      },
      "source": [
        "# Music Transformer Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O-tN3TuVl-ip"
      },
      "source": [
        "___SpectralDoy, May 8 - Aug 26 2020___\n",
        "\n",
        "This notebook builds and trains a Music Transformer decoder model to generate music, based off the description in Huang et. al, 2018 (https://arxiv.org/pdf/1809.04281.pdf), with inferences taken from Shaw et. al, 2018 (Self Attention with Relative Position Representations: https://arxiv.org/pdf/1803.02155.pdf) and Vaswani et. al, 2017 (Attention is All You Need: https://arxiv.org/pdf/1706.03762.pdf), as well as Jay Alammar's blog (http://jalammar.github.io), and using the data representation given by Oore, et. al 2018, (This time with Feeling: https://arxiv.org/pdf/1808.03715.pdf).\n",
        "\n",
        "This project will utilize the TPU provided by Google Colab in order to train the model. However, the GPU or CPU can be used to test it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y-HuOdU_bFiK"
      },
      "source": [
        "# mount gdrive\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/gdrive/\", force_remount=True)\n",
        "\n",
        "PATH = \"./private/path\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qgr3SNOmGVau"
      },
      "source": [
        "# in order to be able to checkpoint while training, connect to GCS Bucket\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "!gcloud config set project-name\n",
        "!gsutil acl ch -u service-495559152420@cloud-tpu.iam.gserviceaccount.com:WRITER gs://bucket"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LNosmnY9bHxx"
      },
      "source": [
        "# things to handle midi files\n",
        "!apt install fluidsynth\n",
        "!cp /usr/share/sounds/sf2/FluidR3_GM.sf2 ./font.sf2 # normal FluidSynth\n",
        "!gsutil -q -m cp gs://magentadata/soundfonts/Yamaha-C5-Salamander-JNv5.1.sf2 /content/ # magenta fluidsynth\n",
        "!pip install midi2audio\n",
        "!pip install mido"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ckD90dLxHJlB"
      },
      "source": [
        "# normal imports\n",
        "%matplotlib inline\n",
        "\n",
        "import tensorflow as tf\n",
        "import numpy as np \n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "import os\n",
        "import math\n",
        "import time\n",
        "import random\n",
        "\n",
        "%cd ./private/path\n",
        "import transformerutil5 as tu\n",
        "%cd ../../.."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gqfSEdTLiy0p"
      },
      "source": [
        "# fancy imports\n",
        "import mido\n",
        "from midi2audio import FluidSynth\n",
        "from IPython.display import Audio"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aIgmlaibGtU3"
      },
      "source": [
        "# set up tpu\n",
        "resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu='grpc://' + os.environ['COLAB_TPU_ADDR'])\n",
        "tf.config.experimental_connect_to_cluster(resolver)\n",
        "tf.tpu.experimental.initialize_tpu_system(cluster_resolver=resolver)\n",
        "tf.config.list_logical_devices('TPU')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EJEs8xwiKvTZ"
      },
      "source": [
        "# tpu distribution strategy\n",
        "strategy = tf.distribute.TPUStrategy(resolver)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pUViFg3UGO4G"
      },
      "source": [
        "## Setup Input Pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uSTtukSUgbOw"
      },
      "source": [
        "Here we will prepare the input to load into the model. This means setting up the Dataset. A key aspect of this is that, while the first (n-1) tokens will be input to the model, it will be asked to predict the last (n-1) tokens. This property can be encoded into the Dataset to be able to make adequate batches.\n",
        "\n",
        "Since the task at hand is generation, we can split the data into train / validation / test in an 8 / 1 / 1 ratio. During training, the validation data will be used to calculate metrics, while the test data will be used as priors for generation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YsZWZOuV29ss"
      },
      "source": [
        "MAX_LENGTH = 1921\n",
        "MAX_REL_DIST = 1921"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7EiXMAAwGUK4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "8ffa8d57-462a-4ea8-b667-1da3397d28f1"
      },
      "source": [
        "data = np.load(PATH + 'maestro_data_1922.npy')\n",
        "\n",
        "# using the ratio 8/1/1, split data into train, val and test\n",
        "lth = data.shape[0]\n",
        "train_len = round(lth * 0.8)\n",
        "val_len = round(lth * 0.1)\n",
        "test_len = round(lth * 0.1)\n",
        "\n",
        "if train_len + val_len + test_len != lth:\n",
        "  test_len += lth - (train_len + val_len + test_len)\n",
        "\n",
        "train_data = data[:train_len]\n",
        "val_data = data[train_len:val_len + train_len]\n",
        "test_data = data[train_len + val_len:]\n",
        "\n",
        "print(f\"There are {lth} files in the data, {train_data.shape[0]} files in the train_data, \"\\\n",
        "      f\"\\n{val_data.shape[0]} files in the validation data, and {test_data.shape[0]} files in the test data.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "There are 302731 files in the data, 242185 files in the train_data, \n",
            "30273 files in the validation data, and 30273 files in the test data.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H1LIC3TUiUti"
      },
      "source": [
        "Now create the datasets and batch the data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rv0wYczjiRwK",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "2b29d045-4663-4e62-f02c-d32db9e03143"
      },
      "source": [
        "BUFFER_SIZE = 120000\n",
        "\n",
        "GLOBAL_BATCH_SIZE = 48\n",
        "per_replica_batch_size = GLOBAL_BATCH_SIZE // strategy.num_replicas_in_sync\n",
        "\n",
        "train_ds = tf.data.Dataset.from_tensor_slices((train_data[:, 1:], train_data[:, :-1]))\n",
        "# drop remainder to be able to distribute on the TPU\n",
        "train_ds = train_ds.shuffle(BUFFER_SIZE).batch(GLOBAL_BATCH_SIZE, drop_remainder=True)\n",
        "# prefetch makes fetching data faster\n",
        "train_ds = train_ds.prefetch(tf.data.experimental.AUTOTUNE) \n",
        "\n",
        "# distribute over TPUs\n",
        "train_dist_ds = strategy.experimental_distribute_dataset(train_ds)\n",
        "\n",
        "num_train_batches = len(list(train_dist_ds))\n",
        "\n",
        "print(f\"There are {num_train_batches} train batches\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "There are 5045 train batches\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uj6-05N4kALP"
      },
      "source": [
        "Here's what a batch of inputs and targets looks like. Notice how the input batch contains start tokens (414) but no end tokens, while the target batch contains no start tokens, but contains end tokens (415), and that all other intermediate tokens are the same in corresponding rows of the input and target batches."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "suQPBQ9pRJrf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "a757117c-3ec2-4314-8f46-d8bd57326f8e"
      },
      "source": [
        "tar_batch, inp_batch = next(iter(train_dist_ds))\n",
        "inp_batch, tar_batch"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(PerReplica:{\n",
              "   0: <tf.Tensor: shape=(6, 1921), dtype=int32, numpy=\n",
              " array([[414,  67, 259, ..., 382,  62, 382],\n",
              "        [414, 382,  51, ..., 400,  40, 259],\n",
              "        [414, 260, 382, ...,  78, 259, 391],\n",
              "        [414, 382,  67, ..., 382,  77, 258],\n",
              "        [414, 258, 382, ...,  75, 259, 382],\n",
              "        [414, 261, 392, ...,  52, 382,  56]], dtype=int32)>,\n",
              "   1: <tf.Tensor: shape=(6, 1921), dtype=int32, numpy=\n",
              " array([[414, 266, 382, ..., 391,  66, 264],\n",
              "        [414,  75, 258, ..., 264, 382,  73],\n",
              "        [414, 258, 394, ...,  54, 258, 396],\n",
              "        [414, 258, 382, ...,  75, 261, 382],\n",
              "        [414, 408,  45, ..., 399,  63, 398],\n",
              "        [414,  74, 258, ..., 382,  87, 262]], dtype=int32)>,\n",
              "   2: <tf.Tensor: shape=(6, 1921), dtype=int32, numpy=\n",
              " array([[414, 382,  86, ...,  90, 259, 382],\n",
              "        [414,  47, 382, ...,  70, 260, 382],\n",
              "        [414,  53, 263, ...,  51, 258, 382],\n",
              "        [414,  68, 260, ..., 382,  52, 265],\n",
              "        [414,  68, 382, ..., 395,  51, 260],\n",
              "        [414, 402,  49, ..., 391,  72, 261]], dtype=int32)>,\n",
              "   3: <tf.Tensor: shape=(6, 1921), dtype=int32, numpy=\n",
              " array([[414, 267, 398, ..., 273, 394,  63],\n",
              "        [414, 382,  41, ...,  49, 289, 391],\n",
              "        [414, 382,  60, ..., 382,  74, 268],\n",
              "        [414,  45, 258, ...,  48, 264, 391],\n",
              "        [414,  62, 258, ..., 382,  69, 401],\n",
              "        [414,  76, 269, ..., 382,  89, 264]], dtype=int32)>,\n",
              "   4: <tf.Tensor: shape=(6, 1921), dtype=int32, numpy=\n",
              " array([[414, 404,  88, ...,  81, 382,  63],\n",
              "        [414, 396,  69, ..., 382,  63, 274],\n",
              "        [414, 382,  67, ...,  45, 262, 382],\n",
              "        [414,  71, 259, ..., 382,  51, 265],\n",
              "        [414, 396,  73, ..., 403,  61, 258],\n",
              "        [414, 382,  51, ...,  83, 274, 382]], dtype=int32)>,\n",
              "   5: <tf.Tensor: shape=(6, 1921), dtype=int32, numpy=\n",
              " array([[414, 270, 382, ...,  78, 266, 403],\n",
              "        [414,  47, 258, ...,  44, 401,  77],\n",
              "        [414,  79, 265, ..., 398,  42, 258],\n",
              "        [414, 266, 396, ..., 382,  55, 258],\n",
              "        [414,  76, 257, ..., 382,  66, 259],\n",
              "        [414, 258, 401, ..., 402,  33, 272]], dtype=int32)>,\n",
              "   6: <tf.Tensor: shape=(6, 1921), dtype=int32, numpy=\n",
              " array([[414,  73, 261, ..., 273, 403,  53],\n",
              "        [414,  66, 264, ...,  71, 260, 382],\n",
              "        [414,  26, 259, ..., 259, 382,  51],\n",
              "        [414, 395,  34, ..., 260, 382,  56],\n",
              "        [414, 403,  68, ..., 382,  82, 258],\n",
              "        [414, 262, 402, ..., 382,  60, 382]], dtype=int32)>,\n",
              "   7: <tf.Tensor: shape=(6, 1921), dtype=int32, numpy=\n",
              " array([[414, 382,  61, ..., 400,  61, 397],\n",
              "        [414, 403,  84, ..., 382,  70, 260],\n",
              "        [414, 296, 394, ..., 397,  67, 260],\n",
              "        [414,  53, 258, ...,  50, 261, 382],\n",
              "        [414, 399,  39, ...,  59, 269, 382],\n",
              "        [414,  67, 259, ...,  41, 270, 382]], dtype=int32)>\n",
              " }, PerReplica:{\n",
              "   0: <tf.Tensor: shape=(6, 1921), dtype=int32, numpy=\n",
              " array([[ 67, 259, 382, ...,  62, 382, 415],\n",
              "        [382,  51, 259, ...,  40, 259, 415],\n",
              "        [260, 382,  61, ..., 259, 391, 415],\n",
              "        [382,  67, 257, ...,  77, 258, 415],\n",
              "        [258, 382,  65, ..., 259, 382, 415],\n",
              "        [261, 392,  68, ..., 382,  56, 415]], dtype=int32)>,\n",
              "   1: <tf.Tensor: shape=(6, 1921), dtype=int32, numpy=\n",
              " array([[266, 382,  50, ...,  66, 264, 415],\n",
              "        [ 75, 258, 382, ..., 382,  73, 415],\n",
              "        [258, 394,  35, ..., 258, 396, 415],\n",
              "        [258, 382,  57, ..., 261, 382, 415],\n",
              "        [408,  45, 258, ...,  63, 398, 415],\n",
              "        [ 74, 258, 400, ...,  87, 262, 415]], dtype=int32)>,\n",
              "   2: <tf.Tensor: shape=(6, 1921), dtype=int32, numpy=\n",
              " array([[382,  86, 258, ..., 259, 382, 415],\n",
              "        [ 47, 382,  59, ..., 260, 382, 415],\n",
              "        [ 53, 263, 400, ..., 258, 382, 415],\n",
              "        [ 68, 260, 400, ...,  52, 265, 415],\n",
              "        [ 68, 382,  56, ...,  51, 260, 415],\n",
              "        [402,  49, 257, ...,  72, 261, 415]], dtype=int32)>,\n",
              "   3: <tf.Tensor: shape=(6, 1921), dtype=int32, numpy=\n",
              " array([[267, 398,  77, ..., 394,  63, 415],\n",
              "        [382,  41, 276, ..., 289, 391, 415],\n",
              "        [382,  60, 300, ...,  74, 268, 415],\n",
              "        [ 45, 258, 382, ..., 264, 391, 415],\n",
              "        [ 62, 258, 401, ...,  69, 401, 415],\n",
              "        [ 76, 269, 401, ...,  89, 264, 415]], dtype=int32)>,\n",
              "   4: <tf.Tensor: shape=(6, 1921), dtype=int32, numpy=\n",
              " array([[404,  88, 258, ..., 382,  63, 415],\n",
              "        [396,  69, 266, ...,  63, 274, 415],\n",
              "        [382,  67, 260, ..., 262, 382, 415],\n",
              "        [ 71, 259, 392, ...,  51, 265, 415],\n",
              "        [396,  73, 258, ...,  61, 258, 415],\n",
              "        [382,  51, 263, ..., 274, 382, 415]], dtype=int32)>,\n",
              "   5: <tf.Tensor: shape=(6, 1921), dtype=int32, numpy=\n",
              " array([[270, 382,  51, ..., 266, 403, 415],\n",
              "        [ 47, 258, 382, ..., 401,  77, 415],\n",
              "        [ 79, 265, 382, ...,  42, 258, 415],\n",
              "        [266, 396,  74, ...,  55, 258, 415],\n",
              "        [ 76, 257, 382, ...,  66, 259, 415],\n",
              "        [258, 401,  93, ...,  33, 272, 415]], dtype=int32)>,\n",
              "   6: <tf.Tensor: shape=(6, 1921), dtype=int32, numpy=\n",
              " array([[ 73, 261, 404, ..., 403,  53, 415],\n",
              "        [ 66, 264, 401, ..., 260, 382, 415],\n",
              "        [ 26, 259, 382, ..., 382,  51, 415],\n",
              "        [395,  34, 395, ..., 382,  56, 415],\n",
              "        [403,  68, 260, ...,  82, 258, 415],\n",
              "        [262, 402,  86, ...,  60, 382, 415]], dtype=int32)>,\n",
              "   7: <tf.Tensor: shape=(6, 1921), dtype=int32, numpy=\n",
              " array([[382,  61, 263, ...,  61, 397, 415],\n",
              "        [403,  84, 400, ...,  70, 260, 415],\n",
              "        [296, 394,  75, ...,  67, 260, 415],\n",
              "        [ 53, 258, 382, ..., 261, 382, 415],\n",
              "        [399,  39, 261, ..., 269, 382, 415],\n",
              "        [ 67, 259, 392, ..., 270, 382, 415]], dtype=int32)>\n",
              " })"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 143
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X9VtCjDE2DU4"
      },
      "source": [
        "Now, the data has been batched and shuffled, ready to input to the model. However, before we can actually build and train the model, we have to define certain functionalities."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A5s3-csWGUqN"
      },
      "source": [
        "## Absolute Positional Encoding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z7y4FzlmGYga"
      },
      "source": [
        "Since the transformer does not use recurrence or convolution, we have to deliberately give it positional information. Though learned relative position embeddings will be added to the model, it is possible that absolute position encoding will aid it in predicting next tokens.\n",
        "\n",
        "The positional encoding vector is added to the embedding vector. Embeddings represent a token in a d-dimensional space where tokens with similar meaning will be closer to each other. But the embeddings do not encode position in the input sequence. So after adding the positional encoding, words will be closer to each other based on the similarity of their meaning, as well as their position in the sentence, in this d-dimensional space - information which the transformer can use to better predict next tokens.\n",
        "\n",
        "The formula for absolute position encoding is as follows:\n",
        "$$\\Large{PE_{(pos, 2k)} = sin(pos / 10000^{2k / d_{model}})} $$\n",
        "$$\\Large{PE_{(pos, 2k+1)} = cos(pos / 10000^{2k / d_{model}})} $$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JZj9XGljkygK"
      },
      "source": [
        "def get_angles(position, k, d_model):\n",
        "  # all values of each k\n",
        "  angle = 1 / np.power(10000, 2 * (k // 2) / d_model)\n",
        "  # matrix multiplied into all positions - represent each position with a d_model sized vector\n",
        "  return position @ angle"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QRLnDBm8nMmI"
      },
      "source": [
        "def abs_positional_encoding(max_position, d_model, n=3):\n",
        "  \"\"\"\n",
        "  returns absolute position encoding, creating a vector representation for all positions\n",
        "  from 0 to max_position of shape (d_model,) -> a matrix of shape (max_position, d_model)\n",
        "  and broadcasts it to n dimensions\n",
        "  \"\"\"\n",
        "  # angles are of shape (positions, d_model)\n",
        "  angles = get_angles(np.arange(max_position)[:, np.newaxis], \n",
        "                      np.arange(d_model)[np.newaxis, :], \n",
        "                      d_model)\n",
        "  \n",
        "  # apply sin to the even indices along the last axis\n",
        "  angles[:, 0::2] = np.sin(angles[:, 0::2])\n",
        "\n",
        "  # apply cos to the odd indices along the last axis\n",
        "  angles[:, 1::2] = np.cos(angles[:, 1::2])\n",
        "\n",
        "  # broadcast to n dimensions\n",
        "  for _ in range(n - 2):\n",
        "    angles = angles[np.newaxis, :]\n",
        "  return tf.cast(angles, tf.float32)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XaqQS6y6s1u7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 382
        },
        "outputId": "b4f11ce0-ba83-465c-a578-f8dd9d7759f1"
      },
      "source": [
        "pos_encoding = abs_positional_encoding(50, 256)\n",
        "print (pos_encoding.shape)\n",
        "\n",
        "fig = plt.figure(figsize=(8, 5.5))\n",
        "plt.pcolormesh(pos_encoding[0], cmap='RdBu')\n",
        "plt.xlabel('Depth')\n",
        "plt.xlim((0, 256))\n",
        "plt.ylabel('Position')\n",
        "plt.colorbar()\n",
        "plt.show()\n",
        "del pos_encoding, fig"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1, 50, 256)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 576x396 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kX19-UtHs_sO"
      },
      "source": [
        "## Masking"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JriNUN4D2HiY"
      },
      "source": [
        "Since some of the input sequences are padded with pad tokens (0), we need to mask out these parts of the input sequences so that the model does not treat it as input. The mask will be created as a tensor of the same shape as the input with ones in the positions that need to be masked.\n",
        "\n",
        "However, the network will be dealing with 3-dimensional or 4-dimensional tensors, rather than a simple 2D embedded sequence. Thus, the shape of the mask must be made broadcastable to n dimensions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2SGGmbOMl5NG"
      },
      "source": [
        "def create_padding_mask(seq, n=4):\n",
        "  \"\"\"\n",
        "  Creates padding mask for a batch of sequences seq. Mask will be of shape\n",
        "  (batch_size, seq_len), and can be broadcasted to n dimensions\n",
        "  \"\"\"\n",
        "  mask = tf.cast(tf.equal(seq, 0), tf.float32) # mask is 1 where seq is 0\n",
        "  # reshape to # batch_size, 1, ..., 1. seq_len\n",
        "  return tf.reshape(mask, (tf.shape(mask)[0], *[1 for _ in range(n-2)], tf.shape(mask)[-1]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vK_CYbhFPoKl",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "354318e2-bd2d-412c-edc9-4aef64c1c294"
      },
      "source": [
        "# for example\n",
        "x = tf.constant([[7, 6, 0, 0, 1], [1, 2, 3, 0, 0], [0, 0, 0, 4, 5]])\n",
        "print(create_padding_mask(x, n=3))\n",
        "del x"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor(\n",
            "[[[0. 0. 1. 1. 0.]]\n",
            "\n",
            " [[0. 0. 0. 1. 1.]]\n",
            "\n",
            " [[1. 1. 1. 0. 0.]]], shape=(3, 1, 5), dtype=float32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e-Wiqxco31Kf"
      },
      "source": [
        "Additionally, in the calculation of Scaled Dot Product Attention, the transformer must be prevented from looking ahead at future tokens, so that the next outputs of the model are based only on the current and previous tokens in the input sequence.\n",
        "\n",
        "This can be achieved by placing an upper triangular mask on the calculated Attention weights. Again, this will be 1 where the attention weights need to be zeroed, and zero otherwise. Unlike the case of the padding mask, where we need to know the values of the input sequences in order to create the mask, here the masking is the same for every input: an upper triangular matrix of ones of shape $(L, L)$ where $L$ is the length of the input sequence. We can create this just with $L$, without needing to input the sequence itself."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "81Sr2D-u2-2W"
      },
      "source": [
        "def create_look_ahead_mask(seq_len):\n",
        "  \"\"\"\n",
        "  Creates an upper triangular mask of ones of shape (seq_len seq_len).\n",
        "  It is the same for all inputs of shape seq_len\n",
        "  \"\"\"\n",
        "  mask = 1 - tf.linalg.band_part(tf.ones((seq_len, seq_len)), -1, 0)\n",
        "  return tf.cast(mask, tf.float32) # (seq_len, seq_len)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4oXabDN86OTG",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 292
        },
        "outputId": "4a144b43-7a9c-4a38-d12d-7a1ed8ef7d10"
      },
      "source": [
        "# example\n",
        "plt.matshow(create_look_ahead_mask(5))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7fda6546afd0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 235
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 288x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ne-eR-ia6iOh"
      },
      "source": [
        "At the stage when we need to input to the decoder, we have to create both a padding mask and a look ahead mask for the input batch. The correct final mask will be the maximum of these two, as the elements that need to be zeroed are represented by 1's, and those that need to be preserved are represented by 0's. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "upX2M5Qo6UrL"
      },
      "source": [
        "def create_mask(inp, n=4):\n",
        "  \"\"\"\n",
        "  function to create the proper mask for an input batch\n",
        "  mask = max(padding_mask, look_ahead_mask)\n",
        "\n",
        "  Args:\n",
        "    inp: batch tensor of input sequences of shape (..., seq_len)\n",
        "  \"\"\"\n",
        "  padding_mask = create_padding_mask(inp, n)\n",
        "  look_ahead_mask = create_look_ahead_mask(inp.shape[-1])\n",
        "\n",
        "  # create final mask\n",
        "  return tf.maximum(padding_mask, look_ahead_mask)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3w05vKrN7aQq",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 253
        },
        "outputId": "5cd91074-de4f-4671-ad6e-47e3af5f430e"
      },
      "source": [
        "# example: create a final decoder mask - columns of same indices where pad_mask is 1 are entirely masked\n",
        "x = tf.constant([[7, 6, 0, 0, 1], [1, 2, 3, 0, 0], [0, 0, 0, 4, 5]])\n",
        "\n",
        "mask = create_mask(x, n=3)\n",
        "\n",
        "fig = plt.figure(figsize=(10, 10))\n",
        "rows = 1\n",
        "cols = 3\n",
        "labels = [\"cols 2, 3\", \"cols 3, 4\", \"cols 1, 2, 3\"]\n",
        "for i in range(rows*cols):\n",
        "  fig.add_subplot(rows, cols, i+1).set_title(\"Masked out upper tri & \" + labels[i])\n",
        "  plt.imshow(mask[i])\n",
        "fig.tight_layout()\n",
        "plt.show()\n",
        "del x, mask, rows, cols, labels"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x720 with 3 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PVm0C-B47jHs"
      },
      "source": [
        "The calculation of attention relies on a softmax of attention weights, and the elements of the attention weights that need to be zeroed are indicated by 1's in the mask. By adding the mask, scaled by an extremely low value such as -1e9 (to mimic -inf), to the attention weights logits, and then softmaxing this sum, the positions at which the mask was 1 will be effectively zeroed."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1VMZWDQt7pLj"
      },
      "source": [
        "## Self-Attention with Relative Position Embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o8dLBVSocbMy"
      },
      "source": [
        "A modification given by Shaw et. al, 2018, improved by Huang et. al, 2018, to the Scaled Dot-Product Attention mechanism given in Vaswani et. al, 2017, which allows the Transformer model to attend to all relevant elements of the input sequences as well as the relative distances between them.\n",
        "\n",
        "$${\\text{RelativeAttention} = \\text{Softmax} \\left(\\frac{QK^\\top + S^{rel}}{\\sqrt{d_{k}}}\\right)V}$$\n",
        "\n",
        "Before attention can be computed, the input batch of sequences $X$ must be embedded to make it of shape $(..., L, d_{model})$, where $d_{model}$ is the size of the embeddings, from simply shape $(..., L)$. For attention to work properly, and for residual connections to be made conveniently, all inputs and outputs that the model deals with are associated with the shape $d_{model}$, which is why it is so named.\n",
        "\n",
        "For instance, the first step in the computation of attention is to _transform_ the input $X$ into 3 representations - the queries($Q$), keys($K$) and values($V$):\n",
        "\n",
        "$$Q = XW^Q$$\n",
        "$$K = XW^K$$\n",
        "$$V = XW^V$$\n",
        "\n",
        "$Q$, $K$, and $V$ are all determined by the parameter matrices $W$, which are learned by backpropagation. In order to preserve the shape of $X$ (i.e., keep $Q, K, V$ all shaped $(..., L, d_{model})$), these $W$ must be of shape $(d_{model}, d_{model})$.\n",
        "\n",
        "Now, the attention function in the transformer takes these three values - $Q$, $K$ and $V$ (and a last one, $E$) - as input. What we want the attention mechanism to achieve is to determines the \"amount\" of \"attention\" that each element in $X$ should pay attention to every other element in $X$ - that is, the importance of different positions of the sequence in constructing the sequence. This is achieved by the compatibility function $QK^\\top + S^{rel}$.\n",
        "\n",
        "Notice that $Q$ is of shape $(..., L, d_{model})$ and $K^\\top$ is of shape $(..., d_{model}, L)$. As a result, the matrix product $QK^\\top$ (and the additional relative position encoding $S^{rel}$) is of shape $(...,L, L)$. Given the goal of the attention mechanism, we would want the $i, j$ element of the output of the compatibility function to represent the amount of attention that the $ith$ element of the input sequence should pay to its $jth$ element. One can imagine that if $W^Q$ and $W^K$ were properly optimized by backpropagation, the representations of the input sequences in $Q$ by $XW^Q$ and in $K$ by $XW^K$ would be such that the matrix product $QK^\\top$ achieves this goal. \n",
        "\n",
        "The softmax turns these compatibilities into probabilities (0 to 1), and it is the softmax of the compatibilities that is multipled by $V$. What does this achieve? Well, the compatibility matrix in itself is virtually useless. All it is is a lookup table of how important element $i$ is to element $j$. By multiplying this compatibility matrix by $V$, what we do is distribute this information about the attention that every element in the sequence should pay to every other element in the sequence into every element of a representation of the input sequence itself. What this also achieves, the softmaxed values being between 0 and 1, is the minimization of unimportant values, and the maximization of the most important ones.\n",
        "\n",
        "However, without altering this configuration properly, elements of the compatibility matrix where $j>i$, would give the $ith$ position of the sequence information about a future position in the sequence, after which the model could simply learn to use this information in order to predict the future tokens, instead of paying attention to its previous and current inputs to do so. This is why the look_ahead_mask must be used on the compatibility matrix. All elements where $j>i$ are simply the upper triangle of the matrix, and so the look_ahead_mask is simply an upper triangular matrix of the same shape.\n",
        "\n",
        "Lastly, the compatibility matrix is scaled by $1/\\sqrt{d_{k}}$ before softmaxing, where $d_k$ is the length of the embeddings for each sequence in $K$ (i.e., shape of last axis), in order to counteract the problem of vanishing gradients when computing softmax (Vaswani et. al, 2017). \n",
        "\n",
        "But where does $E$ come into this, and what is $S^{rel}$?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "as9XtVwAntFb"
      },
      "source": [
        "### Skewing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "os8zEQKew6Q2"
      },
      "source": [
        "While the computation of attention given in Vaswani et. al, 2017 is simply:\n",
        "\n",
        "$${\\text{Attention}(Q, K, V) = \\text{Softmax}\\left(\\frac{QK^\\top}{\\sqrt{d_k}}\\right)V}\\tag{1}$$\n",
        "\n",
        "Shaw et. al, 2018 proposed the addition of a $(..., L, L)$ tensor $S^{rel}$ to the calculation of the softmax logits (as well as to $V$, but we can ignore that one for reasons of space and time complexity) in order to inject information about the relative distances between positions in the sequence into the calculation of attention. One can imagine that without information about the importance of the positions itself, the model would learn how important each element is in the sequence, but not in relation to its actual position in the sequence. This is why positional encoding is necessary. \n",
        "\n",
        "The calculation of $S^{rel}$ was greatly improved on by Huang et. al, 2018:\n",
        "\n",
        "$${S^{rel} = \\text{skew}\\left(QE^\\top\\right)}\\tag{2}$$\n",
        "\n",
        "$Q$ is our queries matrix $XW^Q$, but $E$ is more interesting. $E$ is a set of embeddings for each possible relative distance in the sequence from $-L_{max} + 1$ to 0 (where $L_{max}$ is a maximum relative distance to consider - this has to be predefined). \n",
        "\n",
        "As was stated before, in the computation of the attention logits (the compatibility matrix before being softmaxed), the $i, j$ element describes how much attention the $ith$ position of the sequence should pay to the $jth$ position of the sequence. If we were to inject information about relative position into this calculation with $S^{rel}$, we would therefore want the $i, j$ element of $S^{rel}$ to represent the importance of the relative distance $(j-i)$ for the $ith$ element of the sequence. \n",
        "\n",
        "The way we inject this information is by letting the Relative Position Embeddings $E$ interact with the Queries tensor $Q$, giving rise to a new tensor $S^{rel}$, and then simply adding this information to the vanilla attention logits $(QK^\\top)$. \n",
        "\n",
        "Before this, however, we must slice the last $L$ embeddings from $E$ (the embeddings for relative distances of $-L+1$ to $0$) and to calculate the matrix product $QE^\\top$, or if $L > L_{max}$ (i.e., the input sequence length is greater than the maximum relative distance to consider), we simply use the last relative position embedding (for a relative distance of $-L_{max}+1$) for all indices past this relative distance. This slice is necessary to preserve shape ($QE^\\top$ will be of shape $(..., L, L)$, which is the same shape as $QK^\\top$ and the two can simply be added). Additionally, what we get out of computing this matrix product is the product of every query $Q_i$ with every relative position embedding $E_j$, thus instantiating a tensor whose elements describe the importance of a relative distance of $j$ to the $ith$ element in the sequence.\n",
        "\n",
        "However, simply now adding this matrix product $QE^\\top$ to $QK^\\top$ would not inject the correct relative position information at the required indices, because its $i, j$ element does not descibe the importance of a relative distance of $(j-i)$ to the $ith$ elemtn of the sequence.\n",
        "\n",
        "How do we get around this? First, let's visualize the problem. Consider $E$ to be a set of embedding vectors $E_j$ of length $d_{model}$, ordered from $-L + 1$ to $0$, i.e., $E=(E_{-L+1}, E_{-L+2}, ..., E_0)$, and, for 1 sequence, consider the Queries matrix to be a set of $L$ vectors $Q_i$ each of length $d_{model}$, $Q=(Q_0, Q_1,...,Q_{L-1})$. Then, it is easy to see that the matrix product $QE^\\top$ does not achieve the desired ordering, because the $i, j$ element of $QE^\\top$ does not necessarily incorporate the embedding for a relative distance of $(j-i)$:\n",
        "\n",
        "$$\\begin{equation*}\n",
        "\\pmatrix{Q_0 \\\\ Q_1 \\\\ \\vdots \\\\ Q_{L-2} \\\\ Q_{L-1}} \\cdot\n",
        "\\pmatrix{E_{-L+1} & E_{-L+2} & \\cdots & E_{-1} & E_0} =\n",
        "\\begin{pmatrix}\n",
        "Q_0E_{-L+1} & Q_0E_{-L+2} & \\cdots & Q_0E_{-1} & Q_0E_0 \\\\\n",
        "Q_1E_{-L+1} & Q_1E_{-L+2} & \\cdots & Q_1E_{-1} & Q_1E_0 \\\\\n",
        "\\vdots & \\vdots & \\ddots & \\vdots & \\vdots \\\\\n",
        "Q_{L-2}E_{-L+1} & Q_{L-2}E_{-L+2} & \\cdots & Q_{L-2}E_{-1} & Q_{L-2}E_0 \\\\\n",
        "Q_{L-1}E_{-L+1} & Q_{L-1}E_{-L+2} & \\cdots & Q_{L-1}E_{-1} & Q_{L-1}E_0 \\\\\n",
        "\\end{pmatrix}\n",
        "\\end{equation*}$$\n",
        "\n",
        "Nevertheless, this matrix contains all the information we need to inject this information correctly - every $Q_i$ multiplied by every $E_j$, and we just need to order it so that the $i, j$ element of the matrix is of the form $Q_iE_{j-i}$. Huang et. al, 2018 implemented the skewing algorithm to do this.\n",
        "\n",
        "To understand skewing, imagine the desired ordering. In this ordering, the main diagonal would consist of $Q_iE_0$ - at the $i, i$ position of the matrix, the relative distance to be injected would be 0. Furthermore, along the diagonal just under the main diagonal - the $i, (i-1)$ elements of the matrix - all elements would be $Q_iE_{-1}$. As visible in the above matrix, these elements are given in the last 2 columns - the elements we want along the main diagonal are in the last column, and the elements we want along the diagonal just under it are in the first column from the last.\n",
        "\n",
        "Nevertheless, although we have $L$ elements along the last column, which would fit the main diagonal, we also have $L$ elements in the first column from the last, which is one more than would fit the diagonal just under the main. Nevertheless, if we look closely, we see that we can exclude the first entry, $Q_0E_{-1}$, from this second-to-last column. This is because it has no meaning - the 0th element in the sequence does not have any previous elements, and so cannot have a relative distance of (-1) to any other element of the sequence. Similarly, the first two elements in the third-to-last column of $QE^\\top$, where the queries are multiplied by $E_{-2}$ â $Q_0E_{-2}$ and $Q_1E_{-2}$ â can be ignored, because there are no elements in the sequence that are (-2) positions away from the 0th and 1st positions in the sequence. And so on, so that the last $L-n$ elements of the $nth$ column from the last can fit the $nth$ diagonal under the main.\n",
        "\n",
        "Now, we know where the information we want is (the columns from the last in $QE^\\top$), and we know where we want it to go to make $S^{rel}$ (the diagonals on and under the main). If we could just **skew** the elements of $QE^\\top$ in the columns from the right into the desired diagonals, we would get $S^{rel}$, and we could just add it to $QK^\\top$ in the calculation of the attention logits in order to properly encode relative position. And so, the skewing algorithm was made:\n",
        "1. Pad $QE^\\top$ with a dummy vector of length $L$ to the left\n",
        "2. Reshape the matrix from shape $(..., L, L+1)$ to shape $(..., L+1, L)$\n",
        "3. Slice the last $L$ rows from the second-to-last axis of this tensor - this is $S^{rel}$\n",
        "\n",
        "So finally,\n",
        "\n",
        "$${\\text{RelativeAttention}(Q, K, V, E) = \\text{Softmax} \\left(\\frac{QK^\\top + \\text{skew}\\left(QE^\\top\\right)}{\\sqrt{d_{k}}}\\right)V}\\tag{3}$$\n",
        "\n",
        "Isn't that cool?\n",
        "\n",
        "Another benefit of skewing is that once all the required elements are placed on or below the main diagonal in $\\text{skew}(QE^\\top)$, all irrelevant elements in the upper triangle will be masked out by the look_ahead_mask.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GjrbDLlU8Pr7"
      },
      "source": [
        "def skew(t: tf.Tensor):\n",
        "  \"\"\"\n",
        "  Implements skewing algorithm given by Huang et. al 2018 to reorder the\n",
        "  dot(Q, RelativePositionEmbeddings) matrix into the correct ordering for which\n",
        "  Tij = compatibility of ith query in Q with relative position (j - i)\n",
        "\n",
        "  This implementation accounts for rank n tensors\n",
        "\n",
        "  Algorithm:\n",
        "      1. Pad T\n",
        "      2. Reshape\n",
        "      3. Slice\n",
        "\n",
        "  T is supposed to be of shape (..., L, L), but the function generalizes to any shape\n",
        "  \"\"\"\n",
        "  # pad the input tensor\n",
        "  middle_paddings = [[0, 0] for _ in range(len(t.shape) - 1)]\n",
        "  padded = tf.pad(t, [*middle_paddings, [1, 0]])\n",
        "\n",
        "  # reshape\n",
        "  Srel = tf.reshape(padded, (-1, t.shape[-1] + 1, t.shape[-2]))\n",
        "  Srel = Srel[:, 1:] # slice required positions\n",
        "  return tf.cast(tf.reshape(Srel, t.shape), t.dtype)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IWy8J_1c9L84",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 438
        },
        "outputId": "0d433049-1c63-4c4d-c0e0-a26c32cb77ec"
      },
      "source": [
        "# example\n",
        "u = tf.constant([[0, 1, 1, 0, 2], \\\n",
        "                 [1, 0, 0, 3, 2], \\\n",
        "                 [1, 1, 5, 3, 2], \\\n",
        "                 [0, 7, 5, 3, 2], \\\n",
        "                 [9, 7, 5, 3, 2]], dtype=tf.float32)\n",
        "plots = [u, skew(u)]\n",
        "fig = plt.figure(figsize=(10, 6.5))\n",
        "rows = 1\n",
        "cols = 2\n",
        "labels = ['u', 'skew(u)']\n",
        "fig.suptitle(\"Columns from the right are skewed into diagonals on and under the main, and elements\\n\"\\\n",
        "             \"not in these columns are thrown into the upper triangle and/or replaced by zeros\", \\\n",
        "             fontsize=15) \n",
        "for i in range(rows*cols):\n",
        "  fig.add_subplot(1, 2, i+1).set_title(labels[i], fontsize=14)\n",
        "  plt.imshow(plots[i], cmap='viridis')\n",
        "fig.tight_layout()\n",
        "plt.show()\n",
        "del u, plots, fig, rows, cols, labels"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x468 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RpVltTpV_fwf"
      },
      "source": [
        "### Relative Scaled Dot Product Attention"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "znAaQcoii6lJ"
      },
      "source": [
        "Given the skewing algorithm, we can now define the Relative Attention function. This function is technically called Relative Scaled Dot Product Attention."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sy6f_403XIau"
      },
      "source": [
        "def rel_scaled_dot_prod_attention(q, k, v, e, mask=None):\n",
        "  \"\"\"\n",
        "  Implements equation 3 given in the previous section to calculate the attention weights,\n",
        "  Mask has different shapes depending on its type (padding, look_ahead or combined),\n",
        "  but by scaling and adding it to the attention logits, masking can be performed\n",
        "\n",
        "  Attention = softmax(mask(QKT + skew(QET))/sqrt(d_k))V\n",
        "\n",
        "  Args:\n",
        "    q: Queries matrix of shape (..., seq_len_q, d_model)\n",
        "    k: Keys matrix of shape (..., seq_len_k, d_model)\n",
        "    v: Values matrix of shape (..., seq_len_k, d_model)\n",
        "    e: Relative Position embedding matrix of shape (seq_len_k, d_model)\n",
        "  \n",
        "  Returns:\n",
        "    output attention, attention weights\n",
        "  \"\"\"\n",
        "  QKt = tf.matmul(q, k, transpose_b=True) # (..., seq_len_q, seq_len_k)\n",
        "  Srel = skew(tf.matmul(q, e, transpose_b=True)) # (..., seq_len_q, seq_len_k)\n",
        "\n",
        "  # calculate and scale logits\n",
        "  dk = math.sqrt(k.shape[-1])\n",
        "  scaled_attention_logits = (QKt + Srel) / dk\n",
        "\n",
        "  # add the mask to the attention logits\n",
        "  if mask is not None:\n",
        "    scaled_attention_logits += (mask * -1e09) # mask is added only to attention logits\n",
        "  \n",
        "  # softmax is normalized on the last axis so that the ith row adds up to 1\n",
        "  # this is best for multiplication by v because the last axis (made into \n",
        "  # probabilities) interacts with the values v\n",
        "  attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1) # (..., seq_len_q, seq_len_k)\n",
        "\n",
        "  output = tf.matmul(attention_weights, v) # (..., seq_len_q, d_k)\n",
        "  return output, attention_weights"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HjtgGUL9_AXB"
      },
      "source": [
        "As softmax is performed on the last axis of the attention weights, whose dimensions are determined by $K$, the values in the Keys determine the importance of each Query in $Q$.\n",
        "\n",
        "The (softmaxed) attention weights being multiplied by $V$ to calculate the output ensures that the positions of the queries sequence that you want to focus on are kept, while those that are less important are zeroed out."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hsuPSvFs7gZD",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "82d37b77-8820-419d-a47f-10e438b4ea21"
      },
      "source": [
        "# examples of attention\n",
        "temp_k = tf.constant([[0, 0, 10], [0, 10, 0], [10, 0, 0], [10, 0, 0]], dtype=tf.float32)\n",
        "temp_v = tf.constant([[4, 2, 1], [5, 6, 3], [7, 8, 10], [9, 12, 45]], dtype=tf.float32)\n",
        "temp_e = tf.zeros_like(temp_k) #zero the relative position embeddings to demonstrate original attention\n",
        "\n",
        "temp_q = tf.constant([[0, 10, 0]], dtype=tf.float32)\n",
        "attn, attn_weights = rel_scaled_dot_prod_attention(temp_q, temp_k, temp_v, temp_e)\n",
        "print(\"Attention weights are,\")\n",
        "print(attn_weights)\n",
        "print(\"Output Attention is,\")\n",
        "print(attn)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Attention weights are,\n",
            "tf.Tensor([[8.4332744e-26 1.0000000e+00 8.4332744e-26 8.4332744e-26]], shape=(1, 4), dtype=float32)\n",
            "Output Attention is,\n",
            "tf.Tensor([[5. 6. 3.]], shape=(1, 3), dtype=float32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_BvVoufsYG76"
      },
      "source": [
        "Notice that since ```temp_q``` corresponded to ```\n",
        "temp_k[1]```, the attention weights are maximum at that index, and the output attention is that value in ```temp_v```. The query aligned with a specific key, and thus prioritized the corresponding value to pay attention to."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-RMDOnvVaOFB",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "7aa0e6c3-0d9a-4942-e0a2-f31629c64528"
      },
      "source": [
        "# we should also see how relative position embeddings change the output\n",
        "temp_q = tf.constant([[0, 10, 0]], dtype=tf.float32) # aligns with second key\n",
        "\n",
        "temp_e = tf.constant([[-1, -1, -10], [2, 2, 2], [1, 1, 1], [4, 4, 4]], dtype=tf.float32)\n",
        "\n",
        "attn, attn_weights = rel_scaled_dot_prod_attention(temp_q, temp_k, temp_v, temp_e)\n",
        "print(\"Attention weights are,\")\n",
        "print(attn_weights)\n",
        "print(\"Output Attention is,\")\n",
        "print(attn)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Attention weights are,\n",
            "tf.Tensor([[2.5339164e-33 1.0000000e+00 2.6217687e-28 8.7255939e-21]], shape=(1, 4), dtype=float32)\n",
            "Output Attention is,\n",
            "tf.Tensor([[5. 6. 3.]], shape=(1, 3), dtype=float32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k3mMriKSbiS8"
      },
      "source": [
        "Above, we can see that since the query aligned with the second key, and since the highest embedding is that for relative distance of 0 (the very last embedding vector), this position was prioritized and the second value was output. However, changing ```temp_q``` to align with the first key:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bln27tOEbhej",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "4d26a901-854b-4665-c3df-879708f3a7e1"
      },
      "source": [
        "temp_q = tf.constant([[0, 0, 10]], dtype=tf.float32)\n",
        "\n",
        "temp_v = tf.constant([[4, 2, 1], [5, 6, 3], [7, 8, 10], [9, 12, 45]], dtype=tf.float32)\n",
        "temp_e = tf.constant([[-1, -1, -10], [2, 2, 2], [1, 1, 1], [4, 4, 4]], dtype=tf.float32)\n",
        "\n",
        "attn, attn_weights = rel_scaled_dot_prod_attention(temp_q, temp_k, temp_v, temp_e)\n",
        "print(\"Attention weights are,\")\n",
        "print(attn_weights)\n",
        "print(\"Output Attention is,\")\n",
        "print(attn)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Attention weights are,\n",
            "tf.Tensor([[9.3410162e-11 9.6648464e-06 3.0046529e-08 9.9999034e-01]], shape=(1, 4), dtype=float32)\n",
            "Output Attention is,\n",
            "tf.Tensor([[ 8.999962 11.999942 44.999596]], shape=(1, 3), dtype=float32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YNApJArpeeEs"
      },
      "source": [
        "We see that even though the query aligned with the first key, the output attention is closer to the last value. This is because of the high embedding in the last index of ```temp_e```. Even though it corresponds to a relative distance of 0, the highest value in $S^{rel}$ is the matrix multiplication ```q @ temp_e[-1].transpose```, simply because of the value of that embedding. In order to get the desired output, we have to mask out those values."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mxF8980ueZ4y",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "e82b4032-ff5a-4d20-ba97-f8870f65bc76"
      },
      "source": [
        "attn, attn_weights = rel_scaled_dot_prod_attention(temp_q, temp_k, temp_v, temp_e,\\\n",
        "                                                   mask=create_look_ahead_mask(temp_k.shape[-2])[0])\n",
        "print(\"Attention weights are,\")\n",
        "print(attn_weights)\n",
        "print(\"Output Attention is,\")\n",
        "print(attn)\n",
        "del temp_k, temp_v, temp_e, temp_q, attn, attn_weights"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Attention weights are,\n",
            "tf.Tensor([[1. 0. 0. 0.]], shape=(1, 4), dtype=float32)\n",
            "Output Attention is,\n",
            "tf.Tensor([[4. 2. 1.]], shape=(1, 3), dtype=float32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mv3Bt2McghJs"
      },
      "source": [
        "And we get the first value, as desired. We can also play around with the embeddings to see how we can query one element to get a previous element, or an average of the current and previous elements, depending on the relative position embeddings:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jAhd2uVZhPpi",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "outputId": "9c15e165-ba05-49b5-ab91-e49c22957bd8"
      },
      "source": [
        "# play around with temp_q and temp_e\n",
        "temp_k = tf.constant([[0, 0, 10], [0, 10, 0], [10, 0, 0], [0, 0, 510]], dtype=tf.float32)\n",
        "temp_v = tf.constant([[4, 2, 1], [5, 6, 3], [7, 8, 10], [9, 12, 45]], dtype=tf.float32)\n",
        "\n",
        "temp_q = temp_k\n",
        "\n",
        "# highest embedding is for distance of -2, and second highest is for 0\n",
        "# so for the first 2 values, the distance of -2 is masked out, and it outputs the \n",
        "# positions of relative distance 0\n",
        "# but for the last 2, the values 2 positions behind are output\n",
        "temp_e = tf.constant([[0, 0, 0], [1000000, 1000000, 1000000], [0, 0, 0], [100, 100, 100]], dtype=tf.float32)\n",
        "\n",
        "attn, attn_weights = rel_scaled_dot_prod_attention(temp_q, temp_k, temp_v, temp_e,\\\n",
        "                                                   mask=create_look_ahead_mask(temp_k.shape[-2]))\n",
        "print(\"Attention weights are,\")\n",
        "print(attn_weights)\n",
        "print(\"Output Attention is,\")\n",
        "print(attn)\n",
        "del temp_k, temp_v, temp_e, temp_q, attn, attn_weights"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Attention weights are,\n",
            "tf.Tensor(\n",
            "[[1. 0. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [0. 1. 0. 0.]], shape=(4, 4), dtype=float32)\n",
            "Output Attention is,\n",
            "tf.Tensor(\n",
            "[[4. 2. 1.]\n",
            " [5. 6. 3.]\n",
            " [4. 2. 1.]\n",
            " [5. 6. 3.]], shape=(4, 3), dtype=float32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7GiACiBAZ_R4"
      },
      "source": [
        "In this way, the attention function is able to decide which values in the sequence to pay attention to in order to predict the next tokens, and is aided in this with the correct relative position embeddings."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E3pD5MGDmzPV"
      },
      "source": [
        "### Multi-Head Attention"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SzUHpmS_sTs8"
      },
      "source": [
        "Instead of performing the attention on $d_{model}$-dimensional $Q$, $K$, and $V$, Vaswani et. al, 2017 found it beneficial to compute attention for these tensors along  $h$ different \"heads.\" \n",
        "\n",
        "One can see why this is beneficial simply by looking at the matrix transformations that take place. First, $Q$, $K$, $V$ and $E$ are split to have different parts of their _embeddings_ (not different parts of the sequence) along $h$ different heads. In doing so, each head is given embeddings from each tensor of length $d_h$, where $d_h = \\frac{d_{model}}{h}$:\n",
        "\n",
        "$$\\begin{matrix}\n",
        "Q \\\\\n",
        "K \\\\\n",
        "V\n",
        "\\end{matrix} \\Bigg\\} \\:\\text{shape}(..., L, d_{model}) \\rightarrow \\text{shape}(..., h, L, d_h) $$\n",
        "\n",
        "$E$ is also reshaped from $(L, d_{model})$ to $(h, L, d_h)$. \n",
        "\n",
        "Now, along each of these heads, we compute ```rel_scaled_dot_prod_attention```, and at each head, attention weights of shape $(..., L, L)$ are produced, due to the deliberate reshaping of the input tensors. That is, attention weights are calculated $h$ times (though with resolution downsampled from $d_{model}$ to $d_h$). This means that the model creates $h$ different representations of the amount of attention that each position in each input sequence should pay to every previous position in the sequence. This allows the network to attend to information from several different representations simultaneously, increasing accuracy, as well as making the calculation of attention slightly quicker by computing attention over $h$ smaller tensors in parallel.\n",
        "\n",
        "After multiplication of the attention weights at each head by the $V$ tensor for that head, $V_i, i \\in \\{1...h\\}$, $h$ tensors of shape $(..., L, d_h)$ are created for each sequence in the batch. They can then be concatenated back to a tensor of shape $(..., L, d_{model})$ to get back to the original shaping. Vaswani et. al, 2017 also project this Multi-Head Attention with a final parameter matrix $W^O$ of shape $(d_{model}, d_{model})$ to get the final output. Thus:\n",
        "\n",
        "$$\\text{MultiHeadRelativeAttention} = Concat\\left(\\text{head}_1, ..., \\text{head}_h\\right) W^O \\\\\n",
        "\\text{head}_i = \\text{RelativeAttention}(Q_i, K_i, V_i, E_i)$$\n",
        "\n",
        "As was stated earlier, $Q$, $K$, and $V$ are computed from $X$, the input batch of sequences. We can encode this calculation, as well as the instantiation and use of the Relative Position Embedding Matrix, $E$, into the ```MultiHeadAttention``` block. But first, we must define some helper functions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kBRzV-5QtsbQ"
      },
      "source": [
        "# helper function\n",
        "def split_heads(x, num_heads, depth=None):\n",
        "  \"\"\"\n",
        "  assumes x is of shape (..., num_heads * depth)\n",
        "  split the last dimension of x into (num_heads, depth),\n",
        "  transposes to (..., num_heads, L, depth)\n",
        "  \"\"\"\n",
        "  if depth is None:\n",
        "    assert x.shape[-1] % num_heads == 0\n",
        "    depth = x.shape[-1] // num_heads\n",
        "\n",
        "  # split d_model into h, d_h\n",
        "  x = tf.reshape(x, (*x.shape[:-1], num_heads, depth)) # (..., L, num_heads, depth)\n",
        "\n",
        "  # transpose axes -2 and -3 - tf specifies this with perm so all this fluff needs to be done\n",
        "  final_perm = len(x.shape) - 1\n",
        "  prior_perms = np.arange(0, final_perm - 2) # axes before the ones that need to be transposed\n",
        "\n",
        "  # transpose to shape (..., num_heads, L, depth)\n",
        "  return tf.transpose(x, perm=[*prior_perms, final_perm-1, final_perm-2, final_perm]) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "86hHr9cUudKl",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "5ca6db81-e2c3-4f6c-f047-a22fd5af21da"
      },
      "source": [
        "# test\n",
        "t = tf.random.normal((64, 10, 200))\n",
        "print(split_heads(t, 8, 25).shape)\n",
        "del t"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(64, 8, 10, 25)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fQkJviPCwJG6"
      },
      "source": [
        "# another helper function\n",
        "def get_required_embeddings(E, seq_len, max_len=None):\n",
        "  \"\"\"\n",
        "  Given an input sequence of length seq_len, which does not necessary equal max_len, the \n",
        "  maximum relative distance the model is set to handle, embeddings in E from the right are \n",
        "  the required relative positional embeddings\n",
        "  Embeddings have to be taken from the right because E is considered to be \n",
        "  ordered from -max_len + 1 to 0\n",
        "  For all positions distanced past -max_len + 1, use E_{-max_len + 1}\n",
        "  \"\"\"\n",
        "  if not E.built:\n",
        "    E.build(seq_len)\n",
        "  if max_len is None:\n",
        "    max_len = E.embeddings.get_shape()[0] # assumes E is a keras.layers.Embedding\n",
        "  \n",
        "  if max_len >= seq_len:\n",
        "    seq_len = min(seq_len, max_len)\n",
        "    return E(np.arange(max_len - seq_len, max_len))\n",
        "  \n",
        "  return tf.concat(\n",
        "      values=[*[E(np.arange(0, 1)) for _ in range(seq_len - max_len)], E(np.arange(0, max_len))], \n",
        "      axis=0\n",
        "  )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "02aF4BLxvH-t",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "1d1975b0-f20d-40c5-87d2-e2f6706f63ab"
      },
      "source": [
        "# test\n",
        "E = tf.keras.layers.Embedding(400, 200)\n",
        "print(get_required_embeddings(E, 500).shape)\n",
        "del E"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(500, 200)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N6nFS9fkuksc"
      },
      "source": [
        "Now we can define the ```MultiHeadAttention``` block."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7BoQ_lwlsajJ"
      },
      "source": [
        "class MultiHeadAttention(tf.keras.layers.Layer):\n",
        "  def __init__(self, d_model, num_heads, max_rel_dist=MAX_REL_DIST, use_bias=True):\n",
        "    super(MultiHeadAttention, self).__init__()\n",
        "    self.num_heads = num_heads\n",
        "    self.d_model = d_model\n",
        "    self.max_len = max_rel_dist\n",
        "\n",
        "    assert d_model % num_heads == 0, \"d_model must be divisible into num_heads\"\n",
        "\n",
        "    self.depth = self.d_model // self.num_heads\n",
        "\n",
        "    self.wq = tf.keras.layers.Dense(d_model, use_bias=use_bias) # parameter matrix to generate Q from input\n",
        "    self.wk = tf.keras.layers.Dense(d_model, use_bias=use_bias) # parameter matrix to generate K from input\n",
        "    self.wv = tf.keras.layers.Dense(d_model, use_bias=use_bias) # parameter matrix to generate V from input\n",
        "\n",
        "    self.E = tf.keras.layers.Embedding(self.max_len, self.d_model) # relative position embeddings\n",
        "\n",
        "    self.wo = tf.keras.layers.Dense(d_model, use_bias=use_bias) # final output parameter matrix\n",
        " \n",
        "  def call(self, q, k, v, mask=None):\n",
        "    \"\"\"\n",
        "    Creates Q, K, and V, gets required embeddings in E, splits into heads,\n",
        "    computes attention, concatenates, and passes through final output layer\n",
        "    \"\"\"\n",
        "    # Get Q, K, V\n",
        "    q = self.wq(q) # (batch_size, seq_len, d_model)\n",
        "    k = self.wk(k) # (batch_size, seq_len, d_model)\n",
        "    v = self.wv(v) # (batch_size, seq_len, d_model)\n",
        "    \n",
        "    # Get E\n",
        "    seq_len_k = k.shape[-2]\n",
        "    e = get_required_embeddings(self.E, seq_len_k, self.max_len) # (seq_len_k, d_model)\n",
        "\n",
        "    # split into heads\n",
        "    q = split_heads(q, self.num_heads, self.depth) # (batch_size, h, seq_len_q, depth)\n",
        "    k = split_heads(k, self.num_heads, self.depth) # (batch_size, h, seq_len_k, depth)\n",
        "    v = split_heads(v, self.num_heads, self.depth) # (batch_size, h, seq_len_k, depth)\n",
        "    e = split_heads(e, self.num_heads, self.depth) # (            h, seq_len_k, depth)\n",
        "\n",
        "    # rel_scaled_attention shape = (batch_size, h, seq_len_q, depth)\n",
        "    # attention_weights shape = (batch_size, h, seq_len_q, seq_len_k)\n",
        "    rel_scaled_attention, attention_weights = rel_scaled_dot_prod_attention(q, k, v, e, mask=mask)\n",
        "\n",
        "    # transpose rel_scaled_attention back to (batch_size seq_len_q, h, depth)\n",
        "    final_perm = len(rel_scaled_attention.shape) - 1 # can't use rank for some reason\n",
        "    prior_perms = np.arange(0, final_perm - 2) # axes before the ones that need to be transposed\n",
        "    rel_scaled_attention = tf.transpose(rel_scaled_attention,\n",
        "                                        perm=[*prior_perms, final_perm-1, final_perm-2, final_perm])\n",
        "\n",
        "    # concatenate heads -> (batch_size, seq_len, d_model)\n",
        "    sh = rel_scaled_attention.shape\n",
        "    concat_attention = tf.reshape(rel_scaled_attention, (*sh[:-2], self.d_model)) \n",
        "\n",
        "    output = self.wo(concat_attention)\n",
        "\n",
        "    return output, attention_weights"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gqPrQMQbIQwH",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "de7d6e9c-502e-4f1f-9f87-d8f6552a274b"
      },
      "source": [
        "# Create a MultiHeadAttention Block to test\n",
        "t = tf.random.uniform((10, 1500, 256))\n",
        "mha = MultiHeadAttention(256, 8, use_bias=True)\n",
        "out, attn = mha(t, t, t, create_mask(tf.random.uniform((10, 1500))))\n",
        "\n",
        "print(f\"Shape of the output: {out.shape}\")\n",
        "print(f\"Shape of the attention weights: {attn.shape}\")\n",
        "print(f\"Number of trainable variables in the MHA block: {len(mha.trainable_variables)}\")\n",
        "del t, mha, out, attn"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Shape of the output: (10, 1500, 256)\n",
            "Shape of the attention weights: (10, 8, 1500, 1500)\n",
            "Number of trainable variables in the MHA block: 9\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VA8SXpFPj7O9"
      },
      "source": [
        "Now that we've defined the core mechanism behind the transformer, we can move on to other layers and actually building the model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ws0B9poZkC7t"
      },
      "source": [
        "## Pointwise Feed Forward Network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hUeJ6k2ykIVx"
      },
      "source": [
        "In each layer of the Transformer Decoder, the Multi-Head Attention block is followed by a fully-connected Feed Foward Network, which is simply a 2 layer network with a ReLU activation in between. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2qruafSmqsfh"
      },
      "source": [
        "class PointwiseFFN(tf.keras.layers.Layer):\n",
        "  def __init__(self, d_model, dff, use_bias=True):\n",
        "    super(PointwiseFFN, self).__init__()\n",
        "\n",
        "    self.main = tf.keras.Sequential([\n",
        "        tf.keras.layers.Dense(dff, activation='relu', use_bias=use_bias), # (batch_size, seq_len, dff)\n",
        "        tf.keras.layers.Dense(d_model, use_bias=use_bias) # (batch_size, seq_len, d_model)           \n",
        "    ])\n",
        "  \n",
        "  def call(self, x):\n",
        "    return self.main(x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V3MdmRJDlCt5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "98a7d6a5-873c-4b33-91ad-35f6951c2772"
      },
      "source": [
        "# test it out\n",
        "test_ffn = PointwiseFFN(512, 2048, True)\n",
        "print(f\"Shape of the output: {test_ffn(tf.random.uniform((60, 24, 512))).shape}\")\n",
        "print(f\"Number of trainable variables in the FFN sublayer: {len(list(test_ffn.trainable_variables))}\")\n",
        "del test_ffn"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Shape of the output: (60, 24, 512)\n",
            "Number of trainable variables in the FFN sublayer: 4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BBufulCLlujc"
      },
      "source": [
        "## Decoder Layer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vRaekrdho2n0"
      },
      "source": [
        "While the original Transformer consisted of an Encoder and a Decoder designed for seq2seq tasks, the Transformer Decoder Model was adapted to handle sequence generation. While every Encoder Layer in the original Transformer had 2 sublayers, and every Decoder Layer had 3 sublayers, the Transformer Decoder model, as adapted by Radford et. al, 2019 and others before (such as Liu, et. al, 2018: https://arxiv.org/pdf/1801.10198.pdf), scrapped the Encoder, and consisted solely of a stack of Decoder Layers, each with 2 sublayers:\n",
        "1. Masked Multi-Head Attention\n",
        "2. Pointwise Feed Forward Layer\n",
        "\n",
        "Each sublayer also employs a residual connection followed by a LayerNorm on the last axis."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IpGr1pialNKu"
      },
      "source": [
        "class DecoderLayer(tf.keras.layers.Layer):\n",
        "  def __init__(self, d_model, num_heads, dff, max_rel_dist=MAX_REL_DIST, \n",
        "               use_bias=True, dropout=0.1, layernorm_eps=1e-06):\n",
        "    super(DecoderLayer, self).__init__()\n",
        "\n",
        "    self.mha = MultiHeadAttention(d_model, num_heads, max_rel_dist=max_rel_dist, use_bias=use_bias)\n",
        "    self.ffn = PointwiseFFN(d_model, dff)\n",
        "\n",
        "    self.layernorm1 = tf.keras.layers.LayerNormalization(axis=-1, epsilon=layernorm_eps)\n",
        "    self.layernorm2 = tf.keras.layers.LayerNormalization(axis=-1, epsilon=layernorm_eps)\n",
        "\n",
        "    self.dropout1 = tf.keras.layers.Dropout(dropout)\n",
        "    self.dropout2 = tf.keras.layers.Dropout(dropout)\n",
        "  \n",
        "  def call(self, x, training=False, mask=None):\n",
        "    attn_output, attn_weights = self.mha(x, x, x, mask=mask) # calculate attention\n",
        "    attn_output = self.dropout1(attn_output, training=training) # dropout\n",
        "    # layernorm on residual connection\n",
        "    out1 = self.layernorm1(x + attn_output) # (batch_size, seq_len, d_model)\n",
        "\n",
        "    ffn_output = self.ffn(out1) # pass through FFN\n",
        "    ffn_output = self.dropout2(ffn_output, training=training) # dropout\n",
        "    # layernorm on residual connection\n",
        "    out2 = self.layernorm2(out1 + ffn_output) # (batch_size, seq_len, d_model)\n",
        "\n",
        "    return out2, attn_weights"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DNMM7J8bseaU",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "73c96271-ffe8-4a0a-835b-ef9d233ca84e"
      },
      "source": [
        "# test Decoder Layer\n",
        "t = tf.random.uniform((32, 1500, 256))\n",
        "\n",
        "sample_decoder_layer = DecoderLayer(256, 8, 1024)\n",
        "out, attn = sample_decoder_layer(t, mask=create_look_ahead_mask(t.shape[-2]))\n",
        "\n",
        "print(f\"Shape of the output: {out.shape}\")\n",
        "print(f\"Shape of the attention weights: {attn.shape}\")\n",
        "print(f\"Number of trainable variables in Decoder Layer: {len(list(sample_decoder_layer.trainable_variables))}\")\n",
        "del t, out, sample_decoder_layer, attn"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Shape of the output: (32, 1500, 256)\n",
            "Shape of the attention weights: (32, 8, 1500, 1500)\n",
            "Number of trainable variables in Decoder Layer: 17\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lWrDk6sEubMg"
      },
      "source": [
        "## Transformer Decoder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AbRM5oPdudPu"
      },
      "source": [
        "Now that we have defined the Decoder Layer, we can build the Transformer Decoder as a stack of N decoder layers, along with the functionality to deal with an input sequence of tokens.\n",
        "\n",
        "The Transformer Decoder consists of:\n",
        "1. Input Embedding\n",
        "2. N Decoder Layers\n",
        "3. Final Linear Layer\n",
        "\n",
        "The Input Embedding is the embeddings of size ```vocab_size``` for the input sequence. After the input embedding, absolute position encoding is added. The embedded input sequences are passed into the stack of decoder layers, and the output of that stack is passed into the Final Linear layer to take the decoder output from shape $(..., L, d_{model})$ to ($..., L$, ```vocab_size```). This final layer can be the original input embedding weight matrix, as per Press and Wolf, 2016, (https://arxiv.org/pdf/1608.05859.pdf), as the input and output are from the same vocabulary, or it can be a new Dense layer altogether."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yl39bKxesrtI"
      },
      "source": [
        "class TransformerDecoder(tf.keras.Model):\n",
        "  def __init__(self, num_layers, d_model, num_heads, dff, vocab_size, max_rel_dist=MAX_REL_DIST, \n",
        "               max_abs_position=20000, use_bias=True, dropout=0.1, layernorm_eps=1e-06, tie_emb=False):\n",
        "    super(TransformerDecoder, self).__init__()\n",
        "\n",
        "    self.num_layers = num_layers\n",
        "    self.d_model = d_model\n",
        "    self.tie_emb = tie_emb\n",
        "    self.le = layernorm_eps\n",
        "\n",
        "    self.max_position = max_abs_position # might need for decode\n",
        "\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, d_model) # input embeddings\n",
        "    self.positional_encoding = abs_positional_encoding(max_abs_position, d_model) # absolute position encoding\n",
        "    self.dropout = tf.keras.layers.Dropout(dropout) # embedding dropout\n",
        "\n",
        "    # decoder layers\n",
        "    self.dec_layers = [DecoderLayer(d_model, num_heads, dff, max_rel_dist, use_bias, dropout, layernorm_eps)\\\n",
        "                       for _ in range(self.num_layers)]\n",
        "\n",
        "    # final layer is linear or embedding weight depending on tie emb\n",
        "    if not tie_emb:\n",
        "      self.final_layer = tf.keras.layers.Dense(vocab_size, use_bias=use_bias)\n",
        "  \n",
        "  def call(self, x, training=False, mask=None):\n",
        "    # initialize attention weights dict to output\n",
        "    attention_weights = {}\n",
        "\n",
        "    # embed x and add absolute positional encoding\n",
        "    x = self.embedding(x) # (batch_size, seq_len) -> (batch_size, seq_len, d_model)\n",
        "    x *= math.sqrt(self.d_model) \n",
        "    if self.max_position > 0:\n",
        "      x += self.positional_encoding[:, :x.shape[-2], :]\n",
        "\n",
        "    x = self.dropout(x, training=training)\n",
        "\n",
        "    # pass through decoder layers\n",
        "    for i in range(len(self.dec_layers)):\n",
        "      x, w_attn = self.dec_layers[i](x, training, mask)\n",
        "      attention_weights[f'DecoderLayer{i+1}'] = w_attn\n",
        "    \n",
        "    # final layer\n",
        "    if self.tie_emb:\n",
        "      x = tf.matmul(x, self.embedding.embeddings, transpose_b=True)\n",
        "    else:\n",
        "      x = self.final_layer(x)\n",
        "    \n",
        "    # returns unsoftmaxed logits\n",
        "    return x, attention_weights"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J0aupYs1cQhz",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "cea95161-4ac4-4a84-ede2-61bc00606c27"
      },
      "source": [
        "# example transformer\n",
        "with strategy.scope():\n",
        "  sample_transformer = TransformerDecoder(\n",
        "      num_layers=6, d_model=256, num_heads=8, dff=1024, vocab_size=tu.vocab_size, max_rel_dist=1537, \n",
        "      max_abs_position=20000, use_bias=True, dropout=0.1, tie_emb=True\n",
        "  )\n",
        "out, attn = sample_transformer(tf.random.uniform((16, 1600))) # build the model\n",
        "start = time.time()\n",
        "out, attn = sample_transformer(tf.random.uniform((16, 1600), minval=0, maxval=400, dtype=tf.int32))\n",
        "print(f\"Shape of the output: {out.shape}\")\n",
        "print(f\"Shape of the attention weights: {attn['DecoderLayer1'].shape}\")\n",
        "print(f\"Number of parameters in the Tranformer Decoder: {len(list(sample_transformer.trainable_variables))}\")\n",
        "print(f\"Time taken to compute over an input batch when not training: {time.time()-start} seconds\")\n",
        "del out, attn, sample_transformer, start"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Shape of the output: (16, 1600, 416)\n",
            "Shape of the attention weights: (16, 8, 1600, 1600)\n",
            "Number of parameters in the Tranformer Decoder: 103\n",
            "Time taken to compute over an input batch when not training: 4.357294797897339 seconds\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Ffg4Yh8N9Yi"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C0Q6nRpb7Qgo"
      },
      "source": [
        "## Set Hyperparameters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s0UIMX_t7Si3"
      },
      "source": [
        "Sadly, the TPU can only compute loss on an input batch of batch size 48 and sequence length ~2000 tokens without crashing. The reference paper Huang et. al, 2018 does not specify the batch size but specifies the sequences trained on were of length 2048 tokens. \n",
        "\n",
        "While the batch size and sequence length cannot be changed, other relevant hyperparameters need to be experimented with to get the best results."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vNjs2Llk_oyc"
      },
      "source": [
        "num_layers = 6\n",
        "d_model = 256\n",
        "dff = 1024\n",
        "num_heads = 8\n",
        "\n",
        "max_rel_dist = MAX_REL_DIST\n",
        "max_abs_position = 1\n",
        "\n",
        "use_bias = True\n",
        "tie_emb = False\n",
        "layernorm_eps = 1e-06\n",
        "\n",
        "vocab_size = tu.vocab_size # don't change this\n",
        "dropout_rate = 0.1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eEZpyvQQHkH-"
      },
      "source": [
        "## Learning Rate Schedule"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g49oPpKxHrFm"
      },
      "source": [
        "As per Vaswani et. al, 2017, the Adam optimizer with a custom learning rate scheduler is used when training the transformer model:\n",
        "\n",
        "$$lr = d_{model}^{\\:-0.5} \\cdot \\min{\\left(step\\_num^{-0.5}, step\\_num \\:\\cdot warmup\\_steps^{-1.5} \\right)}$$\n",
        "\n",
        "Additionally, the betas to be used in the Adam optimizer are 0.9 and 0.98, with epsilon equal to 1e-9."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LrF4bVxKHiuL"
      },
      "source": [
        "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
        "  def __init__(self, d_model, warmup_steps=4000):\n",
        "    self.d_model = d_model\n",
        "    self.d_model = tf.cast(self.d_model, tf.float32)\n",
        "\n",
        "    self.warmup_steps = warmup_steps\n",
        "\n",
        "  def __call__(self, step):\n",
        "    arg1 = tf.math.rsqrt(step)\n",
        "    arg2 = step * (self.warmup_steps ** -1.5)\n",
        "    \n",
        "    return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rpbIiGvXIyPv"
      },
      "source": [
        "# how to set up the optimizer\n",
        "learning_rate = CustomSchedule(d_model)\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98, \n",
        "                                     epsilon=1e-09)\n",
        "del learning_rate, optimizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9QFr8iJ5I297",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 296
        },
        "outputId": "f0ec8249-0c46-4af4-fe0d-2d6b14ea960f"
      },
      "source": [
        "plt.plot(CustomSchedule(256)(tf.range(25000, dtype=tf.float32)))\n",
        "plt.ylabel(\"Learning Rate\")\n",
        "plt.xlabel(\"Train Step\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0.5, 0, 'Train Step')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-lBFSxOZKlGH"
      },
      "source": [
        "## Loss and Metrics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dGJe8qXTK8Hl"
      },
      "source": [
        "Since some of the inputs are padded, it is important to apply a padding mask when calculating the loss.\n",
        "\n",
        "The loss object will be Sparse Categorical Entropy Loss. This is the loss to be used when dealing with indices in a vocabulary, rather than one-hot vectors."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q2opTVZYJSSI"
      },
      "source": [
        "with strategy.scope():\n",
        "  loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "      from_logits=True,\n",
        "      reduction=tf.keras.losses.Reduction.NONE\n",
        "  )\n",
        "  def loss_function(target, predictions, criterion=loss_object):\n",
        "    \"\"\"\n",
        "    If defining custom criterion, make sure reduction is none\n",
        "    \"\"\"\n",
        "    mask = tf.not_equal(target, tf.zeros_like(target))\n",
        "    _loss = criterion(target, predictions)\n",
        "\n",
        "    mask = tf.cast(mask, _loss.dtype) # make mask of same dtype as loss\n",
        "    _loss *= mask\n",
        "\n",
        "    return tf.reduce_sum(_loss) / tf.reduce_sum(mask)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aKWOdGTLMePC"
      },
      "source": [
        "with strategy.scope():\n",
        "  train_loss = tf.keras.metrics.Mean(name='train_loss') \n",
        "  train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='train_accuracy')\n",
        "\n",
        "  val_loss = tf.keras.metrics.Mean(name='val loss')\n",
        "  val_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='val accuracy')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uWOeRLyBO1Gn"
      },
      "source": [
        "## Training and Checkpointing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tQ5CT19drJRb"
      },
      "source": [
        "The transformer is an autoregressive model, which means that at the inference stage, it will make next predictions based on its previous outputs.\n",
        "\n",
        "However, while training, we can use teacher forcing - feeding the target into the model as previous output regardless of the true output of the model. This significantly cuts down on the compute required, while usually reducing loss (at the expense of generalizability, nonetheless).\n",
        "\n",
        "Since we are training a generative model, the targets are simply the inputs shifted right by 1 position. The data has already been cut this way during the Input Pipeline.\n",
        "\n",
        "Now, we need to create the model and optimizer, set up the checkpointing mechanism on a GCS Bucket, and train."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aAe25Icxu0ya",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a25c5f48-97cf-42b2-8503-3e2b66ee1f7f"
      },
      "source": [
        "# set before training\n",
        "num_train_steps = 1000000\n",
        "epochs = tf.convert_to_tensor(math.ceil(num_train_steps / num_train_batches))\n",
        "print(epochs)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor(207, shape=(), dtype=int32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G4Dn5KQ8izpP"
      },
      "source": [
        "Create the model and optimizer and the epoch from which to start training."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eovNq0_KrfAH"
      },
      "source": [
        "with strategy.scope():\n",
        "  transformer = TransformerDecoder(\n",
        "      num_layers, d_model, num_heads, dff, vocab_size, MAX_REL_DIST, max_abs_position,\n",
        "      use_bias, dropout_rate, layernorm_eps, tie_emb\n",
        "  )\n",
        "  learning_rate = CustomSchedule(d_model, warmup_steps=4000)\n",
        "\n",
        "  optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98, \n",
        "                                       epsilon=1e-09)\n",
        "  \n",
        "  start_epoch = tf.Variable(0) # to handle restarting training from a checkpoint"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "olfTL7KRAyXQ"
      },
      "source": [
        "Now, since we'll have to train for a while, we'll use checkpoints to train the model. Every ```ckpt_interval``` epochs, we'll save the model weights, the optimizer state and the epoch number. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A3zei4_uTr2J"
      },
      "source": [
        "# build the. model\n",
        "with strategy.scope():\n",
        "  _ = transformer(tf.random.uniform((GLOBAL_BATCH_SIZE, MAX_LENGTH)))\n",
        "  del _"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "35TZO9UJBNQp",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "1dd34f4b-63b9-486a-f1bc-3f017f6ee502"
      },
      "source": [
        "# set up the checkpoints\n",
        "checkpoint_path = \"checkpoint/path/in/bucket\"\n",
        "ckpt_interval = 1 # checkpoint every ckpt_interval epochs\n",
        "\n",
        "with strategy.scope():\n",
        "  checkpoint = tf.train.Checkpoint(transformer=transformer,\n",
        "                                   optimizer=optimizer,\n",
        "                                   epoch=start_epoch)\n",
        "  \n",
        "  ckpt_manager = tf.train.CheckpointManager(checkpoint, checkpoint_path, \n",
        "                                            max_to_keep=5)\n",
        "\n",
        "  if ckpt_manager.latest_checkpoint:\n",
        "    checkpoint.restore(ckpt_manager.latest_checkpoint)\n",
        "    print('Latest checkpoint restored.')\n",
        "    print(f'Training will resume from epoch {start_epoch.numpy()}.')\n",
        "    print(f'{optimizer.iterations.numpy()} train steps have already been completed.')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Latest checkpoint restored.\n",
            "Training will resume from epoch 199.\n",
            "966480 train steps have already been completed.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W6hWetA_u5dd"
      },
      "source": [
        "Now, to train!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hfZqRPIpvLnC"
      },
      "source": [
        "# define the train step and validation step functions\n",
        "def train_step(target, inputs):\n",
        "  # forward pass\n",
        "  with tf.GradientTape() as tape:\n",
        "    predictions, _ = transformer(inputs, training=True, mask=create_mask(inputs))\n",
        "    loss = loss_function(target, predictions) #/ (MAX_LENGTH)\n",
        "  \n",
        "  # update weights\n",
        "  gradients = tape.gradient(loss, transformer.trainable_variables)    \n",
        "  optimizer.apply_gradients(zip(gradients, transformer.trainable_variables))\n",
        "\n",
        "  # accuracy\n",
        "  train_accuracy(target, predictions)\n",
        "  train_loss(loss)\n",
        "  return loss\n",
        "\n",
        "def val_step(target, inputs):\n",
        "  # forward pass\n",
        "  predictions, _ = transformer(inputs, training=True, mask=create_mask(inputs))\n",
        "  loss = loss_function(target, predictions) #/ (MAX_LENGTH)\n",
        "\n",
        "  # accuracy\n",
        "  val_accuracy(target, predictions)\n",
        "  val_loss(loss)\n",
        "\n",
        "  return loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SnNK32QMPcSk"
      },
      "source": [
        "Since the step functions return the loss computed at each TPU core, we need to reduce the distributed loss back into one value."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B3nkx4zO2shf"
      },
      "source": [
        "# distributed steps\n",
        "@tf.function\n",
        "def distributed_train_step(dataset_inp):\n",
        "  per_replica_losses = strategy.run(train_step, args=(dataset_inp))\n",
        "  final_loss =  strategy.reduce(tf.distribute.ReduceOp.SUM, per_replica_losses, axis=None)\n",
        "\n",
        "  # get metrics\n",
        "  train_loss(final_loss)\n",
        "\n",
        "  return final_loss\n",
        "\n",
        "@tf.function\n",
        "def distributed_val_step(dataset_inp):\n",
        "  per_replica_losses = strategy.run(val_step, args=(dataset_inp))\n",
        "  final_loss = strategy.reduce(tf.distribute.ReduceOp.SUM, per_replica_losses, axis=None)\n",
        "\n",
        "  # get metrics\n",
        "  val_loss(final_loss)\n",
        "\n",
        "  return final_loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OlRF0bie5_X2"
      },
      "source": [
        "Finally, the training loop!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cm8_COh_cjS5"
      },
      "source": [
        "# train loop\n",
        "try:\n",
        "  for epoch in range(start_epoch.numpy(), epochs):\n",
        "    start = time.time()\n",
        "    batch_timer = time.time()\n",
        "    \n",
        "    # train steps\n",
        "    train_loss.reset_states()\n",
        "    train_accuracy.reset_states()\n",
        "\n",
        "    for batch, ds_inp in enumerate(train_dist_ds):\n",
        "      distributed_train_step(ds_inp)\n",
        "\n",
        "      if (batch + 1) % 202 == 0 or batch == 0:\n",
        "        print(f\"Processing Epoch {epoch} Train Batch {batch} \" \\\n",
        "              f\"Loss {round(train_loss.result().numpy().item(), 6)} \" \\\n",
        "              f\"Accuracy {round(train_accuracy.result().numpy().item(), 6)} \" \\\n",
        "              f\"Time taken {round(time.time() - batch_timer, 2)} secs\")\n",
        "        batch_timer = time.time()\n",
        "    \n",
        "    batch_timer = time.time()\n",
        "    \n",
        "    if (epoch + 1) % ckpt_interval == 0:\n",
        "      start_epoch.assign(epoch + 1)\n",
        "      print(\"Checkpointing...\", end=\"\")\n",
        "      save_path = ckpt_manager.save()\n",
        "      print(f\"Done! Saved at {save_path}\")\n",
        "\n",
        "    print(f\"Epoch {epoch} \"\\\n",
        "          f\"Train Loss {round(train_loss.result().numpy().item(), 6)} \" \\\n",
        "          f\"Train Accuracy {round(train_accuracy.result().numpy().item(), 6)}\", end=\" \")\n",
        "    print(f\"Val Loss {round(val_loss.result().numpy().item(), 6)} \"\\\n",
        "          f\"Val Accuracy {round(val_accuracy.result().numpy().item(), 6)}\")\n",
        "    print(f\"Time taken for 1 epoch {round(time.time() - start, 2)} secs\\n\")\n",
        "\n",
        "except KeyboardInterrupt:\n",
        "  print(\"\\nKeyboard Interrupt\")\n",
        "  print(f\"{optimizer.iterations.numpy()} train steps have been computed.\")\n",
        "  print(f\"Current Train Loss {round(train_loss.result().numpy().item(), 6)} and \" \\\n",
        "        f\"Train Accuracy {round(train_accuracy.result().numpy().item(), 6)} \\n\"\n",
        "        f\"Current Val Loss {round(val_loss.result().numpy().item(), 6)} and \"\\\n",
        "        f\"Val Accuracy {round(val_accuracy.result().numpy().item(), 6)}\\n\")\n",
        "  save = input(\"Save the model?\\n\")\n",
        "  if save == 'y' or save == 'yes':\n",
        "    model_save_path = PATH + f\"Models/1920_model_{optimizer.iterations.numpy()}_train_steps.h5\"\n",
        "    print(f\"Saving at {model_save_path}...\", end=\"\")\n",
        "    transformer.save_weights(model_save_path)\n",
        "    print(\"Done!\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EpiKxNzCIGRX"
      },
      "source": [
        "## Saving and Loading"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-hyUd406IIIR"
      },
      "source": [
        "Now that the transformer has been trained, we can save it with the ```save_weights``` function. Note that we cannot save the model as it is a custom subclassed model, and that the save format must be h5."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i3iQX9hk9hcE"
      },
      "source": [
        "model_save_path = PATH + \"Models/\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zpjUQBpaV0mb"
      },
      "source": [
        "transformer.save_weights(model_save_path + f\"1920_model_{optimizer.iterations.numpy()}_train_steps.h5\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KXuE0jKWWGyJ"
      },
      "source": [
        "Now, we can load the model as follows, with or without a strategy. Since we have saved only the weights, we have to build the model before the weights can be loaded."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b4S6og7wWGYT"
      },
      "source": [
        "# create the model\n",
        "transformer = TransformerDecoder(\n",
        "    num_layers, d_model, num_heads, dff, vocab_size, MAX_LENGTH, max_abs_position,\n",
        "    use_bias, dropout_rate, layernorm_eps, tie_emb\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0pe6nUOGWvD4"
      },
      "source": [
        "# build the model\n",
        "_ = transformer(tf.random.uniform((2, MAX_LENGTH)))\n",
        "del _"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oojK2QlBWdsa"
      },
      "source": [
        "# load the weights\n",
        "transformer.load_weights(model_save_path + \"1920_model_1006840_train_steps.h5\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3E4GjQRwiF-o"
      },
      "source": [
        "## Generate!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WTiSnOXpiUQW"
      },
      "source": [
        "Once the model is trained, we have to define a decoding function for the model to autoregressively compute its outputs. Note that in order to generate outputs much quicker without having to use a distribution strategy, change to GPU runtime. It takes about 90 seconds to generate 1500 tokens.\n",
        "\n",
        "The decode function will let the model generate until it predicts an end token. Since the model can only take a fixed length of inputs at a time, this means iteratively appending and clipping the input to the model, but storing the outputs at every step."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CJDOkiqVATY2"
      },
      "source": [
        "def greedy_decode(transformer, inp, mode='categorical', temperature=1.0, k=None, skip_ends=0, memory=1000):\n",
        "  \"\"\"\n",
        "  Decodes inp greedily by appending last outputs to the input and feeding\n",
        "  back into the model. Model is made to generate until end token is predicted\n",
        "  by feeding only the last model.max_len inputs to the model at each decode step\n",
        "  \"\"\"\n",
        "  # get tokens\n",
        "  if not isinstance(inp, tf.Tensor) and not isinstance(inp, np.ndarray):\n",
        "    inp = tu.events_to_indices(inp)\n",
        "  if inp[0] != tu.start_token:\n",
        "    middle_dims = [[0, 0] for _ in range(tf.rank(inp) - 1)]\n",
        "    inp = tf.pad(inp, paddings=[*middle_dims, [1, 0]], constant_values=tu.start_token)\n",
        "  # check if temperature / k is a function\n",
        "  if not callable(temperature):\n",
        "    temperature_ = temperature; del temperature\n",
        "    temperature = lambda x: temperature_\n",
        "\n",
        "  if not callable(k) and k is not None:\n",
        "    k_ = k; del k\n",
        "    k = lambda x: k_\n",
        "\n",
        "  # dimension for the mask\n",
        "  n = tf.rank(inp) + 2 if tf.rank(inp) > 0 else 3\n",
        "\n",
        "  # make inp 2d\n",
        "  inp = [tf.expand_dims(inp, 0)]\n",
        "\n",
        "  # initialize attention weights in case inp.shape[-1] is already > max_len\n",
        "  attention_weights = {}\n",
        "\n",
        "  # maximum number of tokens to input to the model\n",
        "  try:\n",
        "    while True:\n",
        "      predictions, attention_weights = transformer(inp[-1], training=False, \n",
        "                                                   mask=create_mask(inp[-1], n))\n",
        "\n",
        "      # divide logits by temperature\n",
        "      predictions /= temperature(inp[-1].shape[-1])\n",
        "\n",
        "      # get last prediction\n",
        "      if mode == 'argmax' or mode == 'a':\n",
        "        prediction = tf.expand_dims(tf.argmax(predictions[..., -1, :], axis=-1, output_type=tf.int32), 0)\n",
        "      elif k is not None:\n",
        "        top_k_final_predictions = tf.math.top_k(predictions[..., -1, :], \n",
        "                                                k=k(inp[-1].shape[-1]))\n",
        "        predicted_idx = tf.random.categorical(\n",
        "            logits=top_k_final_predictions.values, \n",
        "            num_samples=1,\n",
        "            dtype=tf.int32\n",
        "        )\n",
        "        predicted_idx = tf.squeeze(predicted_idx)\n",
        "        prediction = tf.expand_dims(tf.expand_dims(top_k_final_predictions.indices[0, predicted_idx], 0), 0)\n",
        "      elif mode == 'categorical' or mode == 'c':\n",
        "        prediction = tf.random.categorical(logits=predictions[..., -1, :], num_samples=1, dtype=tf.int32)\n",
        "      else:\n",
        "        print(f\"Unsupported mode '{mode}'. Use 'argmax' or 'categorical'\")\n",
        "        return None\n",
        "    \n",
        "      # return if prediction is end token\n",
        "      if prediction == tu.end_token: #or inp[-1].shape[-1] == MAX_LENGTH:\n",
        "        if skip_ends <= 0:\n",
        "          out = tf.concat(inp, axis=-1)\n",
        "          return tf.squeeze(out)[1:], attention_weights\n",
        "        else:\n",
        "          skip_ends -= 1\n",
        "          vec = inp[-1]\n",
        "          inp.append(vec[:, :-memory])\n",
        "          # maybe i need to put the start token here so that it actually ends at 1920 positions\n",
        "          inp.append(vec[:, -memory:])\n",
        "          inp = inp[:-3] + inp[-2:]\n",
        "      # else concatenate last output to inp\n",
        "      inp[-1] = tf.concat([inp[-1], prediction], axis=-1)\n",
        "  except KeyboardInterrupt:\n",
        "    pass\n",
        "  out = tf.concat(inp, axis=-1)\n",
        "  return tf.squeeze(out)[1:], attention_weights"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fPGb8KRLugRJ"
      },
      "source": [
        "Now, before executing that function, we can also use fluidsynth and IPython.display to turn the generated output into playable .wav files. To do so, however, we can define two new functions - one to save the generated .midi file and convert it to a .wav file and save it again in the specified path; and a second to simply combine this function with the greedy decode function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z9Qwe5N1a7q1"
      },
      "source": [
        "def audiate(idx_list, path='./bloop.mid', tempo=512820, gain=1.0, sr=44100, wav=True, verbose=False):\n",
        "  # check path is mid or midi, set to mid, else invalid path\n",
        "  if path.endswith(\"midi\"):\n",
        "    path = path[:-1]\n",
        "  elif path.endswith(\"mid\"):\n",
        "    pass\n",
        "  else:\n",
        "    print(\"Invalid extension. Use '.mid' or '.midi'.\")\n",
        "    return None          \n",
        "  \n",
        "  # create and save the midi file\n",
        "  print(\"Saving midi file...\") if verbose else None\n",
        "  mid = tu.Listparser(index_list=idx_list, tempo=tempo)\n",
        "  mid.save(path)\n",
        "  if not wav:\n",
        "      print(f\"Midi saved at {path}\")\n",
        "      return None\n",
        "  # run the FluidSynth command - could also use font.sf2\n",
        "  print(\"Creating wav file...\\n\") if verbose else None\n",
        "  os.system(f\"fluidsynth -ni Yamaha-C5-Salamander-JNv5.1.sf2 {path} -F {path[:-4]}.wav -r 44100 -g {gain}\")\n",
        "  \n",
        "  return Audio(f\"{path[:-4]}.wav\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b3rH0ieculDT"
      },
      "source": [
        "def generate(transformer, inp, path='./bloop.mid', mode='categorical', temperature=1.0, \n",
        "             k=None, skip_ends=0, memory=1000, tempo=512820, wav=True, verbose=False):\n",
        "  # get the index list\n",
        "  if verbose:\n",
        "    print(\"Greedy decoding...\", end='')\n",
        "    start = time.time()\n",
        "    idx_list, attn_weights = greedy_decode(transformer, inp, mode, temperature, \n",
        "                                           k, skip_ends, memory)\n",
        "    end = time.time()\n",
        "    print(f\"Generated {len(idx_list)} tokens.\", end=\" \")\n",
        "    print(f\"Time taken: {round(end - start, 2)} secs.\")\n",
        "  else:\n",
        "    idx_list, attn_weights = greedy_decode(transformer, inp, mode, temperature, \n",
        "                                           k, skip_ends, memory)\n",
        "  # generate audio\n",
        "  return audiate(idx_list, path, tempo, wav=wav, verbose=verbose)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "emVcBIqt7Shb"
      },
      "source": [
        "generate(transformer, ['<start>'], k=30, tempo=600000, verbose=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LBHlixGkygrC"
      },
      "source": [
        "## Conclusion and Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cpvv4GVaUgyD"
      },
      "source": [
        "The model was trained for a little over 1 million train steps on the full MAESTRO dataset, represented with the vocabulary given in Oore et. al, 2018, augmented by 5 pitch transpositions and 2 time stretches, and cut into 1920-token-long excerpts. The final training cross-entropy loss achieved was 1.257 with a final training accuracy of 57.01%. The model was clearly overfitting about 100,000 train steps in, and so, calculation of validation metrics was stopped at this point to speed up training.\n",
        "\n",
        "\\\\\n",
        "Several good things can be said about the results of the model. At times, the generated audio (with variable temperature and top-k sampling) is melodically consistent, containing repeated phrases and mostly sticking to classical harmony (mostly using notes of the tonic key with small deviations to create interest, etc.). Tonality as well as virtuosic tendencies (double octaves, fast scales, broken chords, etc.) present in the generated audio, show that the model was able to extract these features from the data and learn how they are used in Western classical music.\n",
        "\n",
        "\\\\\n",
        "Still, the model has several flaws. While the generated samples exhibit a bit of \"expressive\" playing and parts that \"sound nice\", they are quite prone to inconsistent and wandering melodies, sudden changes in rhythm, a high degree of repetition, long pauses, and staccato notes. Perhaps the most obvious flaw is the model's capacity to forget the melody it started with, failing to incorporate past information into the production of new information, something which the Transformer is specifically designed to do.\n",
        "\n",
        "\\\\\n",
        "Still, this project was a great exercise, and I learned a lot. I hope it leads to even better things.\n"
      ]
    }
  ]
}